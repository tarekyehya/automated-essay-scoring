{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8029842,"sourceType":"datasetVersion","datasetId":4732809}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Libraries","metadata":{}},{"cell_type":"code","source":"!pip install datasets==2.15","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-29T20:21:07.074060Z","iopub.execute_input":"2024-04-29T20:21:07.074751Z","iopub.status.idle":"2024-04-29T20:23:36.833313Z","shell.execute_reply.started":"2024-04-29T20:21:07.074715Z","shell.execute_reply":"2024-04-29T20:23:36.832153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing necessary libraries\nimport nltk\nimport numpy as np # linear algebra\nimport lightgbm as lgb\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import cohen_kappa_score\nfrom lightgbm import log_evaluation, early_stopping\nfrom sklearn.linear_model import SGDClassifier\nimport polars as pl\nimport torch\nfrom IPython.display import display\nfrom datasets import Dataset,DatasetDict\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\n\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:23:36.835227Z","iopub.execute_input":"2024-04-29T20:23:36.835574Z","iopub.status.idle":"2024-04-29T20:24:09.181557Z","shell.execute_reply.started":"2024-04-29T20:23:36.835546Z","shell.execute_reply":"2024-04-29T20:24:09.180694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reading the Data","metadata":{}},{"cell_type":"code","source":"columns = [  \n    (\n        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n    ),\n]\nPATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n# 载入训练集和测试集，同时对full_text数据使用\\n\\n字符分割为列表，重命名为paragraph\n# Load training and testing sets, while using \\ n \\ n character segmentation to list and renaming to paragraph for full_text data\ntrain = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\n\n# for test only\n#train = train.sample(200)\ntest = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n# 显示训练集中的第一个样本数据\n# Display the first sample data in the training set\ntrain.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:24:09.182623Z","iopub.execute_input":"2024-04-29T20:24:09.183253Z","iopub.status.idle":"2024-04-29T20:24:09.974427Z","shell.execute_reply.started":"2024-04-29T20:24:09.183226Z","shell.execute_reply":"2024-04-29T20:24:09.973464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing ","metadata":{}},{"cell_type":"code","source":"def removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\ndef dataPreprocessing(x):\n    # 将单词转化为小写\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    # 移除html\n    x = removeHTML(x)\n    # 删除以@作为首字母的字符串\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # 删除数字\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x) # can delete it\n    x = re.sub(\"\\d+\", '',x)\n    # 删除网址\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # 将连续空白符替换为一个空格字符\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # 替换连续的句号和逗号为一个\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # 去除开头结尾的空白符\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:24:09.976570Z","iopub.execute_input":"2024-04-29T20:24:09.976867Z","iopub.status.idle":"2024-04-29T20:24:09.984277Z","shell.execute_reply.started":"2024-04-29T20:24:09.976840Z","shell.execute_reply":"2024-04-29T20:24:09.983364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature extraction","metadata":{}},{"cell_type":"code","source":"# convert the df to dataset \ntrain = train.with_columns(pl.col('full_text').map_elements(dataPreprocessing,return_dtype=str))\ntest = test.with_columns(pl.col('full_text').map_elements(dataPreprocessing,return_dtype=str))\n\nds_train = Dataset.from_pandas(train.to_pandas())\nds_test = Dataset.from_pandas(test.to_pandas())\n\nds = DatasetDict({\"train\": ds_train, \"test\": ds_test})\n\n# Print the dataset dictionary keys and sizes\nfor name, dataset in ds.items():\n    print(f\"{name} size:\", len(dataset))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:24:09.985525Z","iopub.execute_input":"2024-04-29T20:24:09.985899Z","iopub.status.idle":"2024-04-29T20:24:10.132836Z","shell.execute_reply.started":"2024-04-29T20:24:09.985863Z","shell.execute_reply":"2024-04-29T20:24:10.131904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenize \nmodel_ckpt = \"/kaggle/input/es-deberta-large-fold0\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\ndef tokenize(batch):\n    return tokenizer(batch[\"full_text\"], padding=True)\n\ndst = ds.map(tokenize, batched=True ,batch_size = 64)\nprint(dst)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:24:10.134112Z","iopub.execute_input":"2024-04-29T20:24:10.134390Z","iopub.status.idle":"2024-04-29T20:24:10.815152Z","shell.execute_reply.started":"2024-04-29T20:24:10.134367Z","shell.execute_reply":"2024-04-29T20:24:10.814291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModel.from_pretrained(model_ckpt).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:24:10.816211Z","iopub.execute_input":"2024-04-29T20:24:10.816600Z","iopub.status.idle":"2024-04-29T20:24:25.273187Z","shell.execute_reply.started":"2024-04-29T20:24:10.816574Z","shell.execute_reply":"2024-04-29T20:24:25.272347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set format to tensors, ( needed for the model Feedforward )\ndst.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\"])\n\n# remove unwanted features\ncolumns_to_remove = ['full_text', 'paragraph']\ndstr = dst.remove_columns(columns_to_remove)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:24:25.274395Z","iopub.execute_input":"2024-04-29T20:24:25.275006Z","iopub.status.idle":"2024-04-29T20:24:25.285389Z","shell.execute_reply.started":"2024-04-29T20:24:25.274979Z","shell.execute_reply":"2024-04-29T20:24:25.284464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract the features\ndef extract_hidden_states(batch):\n    # Place model inputs on the GPU\n    inputs = {k:v.to(device) for k,v in batch.items() \n              if k in tokenizer.model_input_names}\n    # Extract last hidden states\n    with torch.no_grad():\n        last_hidden_state = model(**inputs).last_hidden_state\n    # Return vector for [CLS] token\n    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\n\n# prepare the vectors\ndstr_hidden = dstr.map(extract_hidden_states,batched=True,batch_size = 2)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:24:25.286529Z","iopub.execute_input":"2024-04-29T20:24:25.286793Z","iopub.status.idle":"2024-04-29T20:25:17.274919Z","shell.execute_reply.started":"2024-04-29T20:24:25.286771Z","shell.execute_reply":"2024-04-29T20:25:17.273845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df = pd.DataFrame(dstr_hidden['train']['hidden_state'].numpy())\nfeatures_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:49:18.573653Z","iopub.execute_input":"2024-04-29T20:49:18.574023Z","iopub.status.idle":"2024-04-29T20:49:18.608359Z","shell.execute_reply.started":"2024-04-29T20:49:18.573989Z","shell.execute_reply":"2024-04-29T20:49:18.607459Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df['essay_id'] = dstr_hidden['train']['essay_id']\nfeatures_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T20:51:12.164661Z","iopub.execute_input":"2024-04-29T20:51:12.165020Z","iopub.status.idle":"2024-04-29T20:51:12.189964Z","shell.execute_reply.started":"2024-04-29T20:51:12.164990Z","shell.execute_reply":"2024-04-29T20:51:12.189062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df.to_csv(\"features_deberta.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]}]}