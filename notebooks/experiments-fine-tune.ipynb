{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":4620664,"sourceType":"datasetVersion","datasetId":2663421},{"sourceId":8575516,"sourceType":"datasetVersion","datasetId":5127900}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### what we will do:\n    - full fine tune \n    - analysis layers to  know which layers i can use LoRa with\n    - test another ways.\n","metadata":{}},{"cell_type":"code","source":"!pip install bitsandbytes==0.43.1\n!pip install accelerate==0.30.1\n!pip install peft==0.11.1\n!pip install transformers==4.41.2","metadata":{"execution":{"iopub.status.busy":"2024-06-22T07:31:32.399833Z","iopub.execute_input":"2024-06-22T07:31:32.400163Z","iopub.status.idle":"2024-06-22T07:32:39.379672Z","shell.execute_reply.started":"2024-06-22T07:31:32.400140Z","shell.execute_reply":"2024-06-22T07:32:39.378577Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes==0.43.1\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.43.1) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes==0.43.1) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes==0.43.1) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes==0.43.1) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes==0.43.1) (1.3.0)\nDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.1\nCollecting accelerate==0.30.1\n  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (0.22.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.30.1) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.30.1) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.30.1) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.30.1) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.30.1) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.30.1) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.30.1) (1.3.0)\nDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.28.0\n    Uninstalling accelerate-0.28.0:\n      Successfully uninstalled accelerate-0.28.0\nSuccessfully installed accelerate-0.30.1\nCollecting peft==0.11.1\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (4.39.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (0.30.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (0.4.2)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.11.1) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.11.1) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.11.1) (0.15.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.11.1) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.11.1) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.11.1\nCollecting transformers==4.41.2\n  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (3.13.1)\nCollecting huggingface-hub<1.0,>=0.23.0 (from transformers==4.41.2)\n  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (2.31.0)\nCollecting tokenizers<0.20,>=0.19 (from transformers==4.41.2)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.41.2) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.41.2) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.41.2) (2024.2.2)\nDownloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.22.2\n    Uninstalling huggingface-hub-0.22.2:\n      Successfully uninstalled huggingface-hub-0.22.2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.39.3\n    Uninstalling transformers-4.39.3:\n      Successfully uninstalled transformers-4.39.3\nSuccessfully installed huggingface-hub-0.23.4 tokenizers-0.19.1 transformers-4.41.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport datasets\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n\n# When using PEFT, comment out the below line.\nfrom peft import LoftQConfig, LoraConfig, TaskType, get_peft_model, PeftModel, PeftConfig","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-22T07:32:39.381966Z","iopub.execute_input":"2024-06-22T07:32:39.382361Z","iopub.status.idle":"2024-06-22T07:32:58.613328Z","shell.execute_reply.started":"2024-06-22T07:32:39.382326Z","shell.execute_reply":"2024-06-22T07:32:58.612552Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-22 07:32:47.649548: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-22 07:32:47.649679: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-22 07:32:47.760641: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"class CFG:\n    n_labels = 6\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    seed = 1\n    \n    # ----- Model checkpoint -----\n    #model_ckpt = '/kaggle/input/deberta-v3-for-offline/base'\n    #model_ckpt = '/kaggle/input/huggingfacedebertav3variants/deberta-v3-base'\n    model_ckpt = 'microsoft/deberta-v3-base' # When 'INTERNET ON'\n    \n    # ----- Training params -----\n    max_input_length = 2000\n    use_peft = False\n    rank = 32\n    n_freeze = False\n    n_folds = 4 \n    learning_rate = 5.0e-5\n    warmup_ratio = 0.1\n    n_epochs = 2\n    train_batch_size = 4\n    eval_batch_size = 2\n    grad_accum_steps = 4\n    steps = 200\n    fp16 = True\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T07:32:58.614412Z","iopub.execute_input":"2024-06-22T07:32:58.614961Z","iopub.status.idle":"2024-06-22T07:32:58.621148Z","shell.execute_reply.started":"2024-06-22T07:32:58.614936Z","shell.execute_reply":"2024-06-22T07:32:58.620158Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Data","metadata":{}},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/'\ndf = pd.read_csv(DATA_DIR + 'train.csv')\n\n# score: [1,2,3,4,5,6] -> label: [0,1,2,3,4,5]\ndf['label'] = df['score'].apply(lambda x: int(x - 1)).astype('uint8')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T07:32:58.623844Z","iopub.execute_input":"2024-06-22T07:32:58.624268Z","iopub.status.idle":"2024-06-22T07:32:59.374100Z","shell.execute_reply.started":"2024-06-22T07:32:58.624244Z","shell.execute_reply":"2024-06-22T07:32:59.373294Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# sample of the data for debuging\n\ndf = df.sample(4000).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T07:32:59.375671Z","iopub.execute_input":"2024-06-22T07:32:59.376178Z","iopub.status.idle":"2024-06-22T07:32:59.390474Z","shell.execute_reply.started":"2024-06-22T07:32:59.376137Z","shell.execute_reply":"2024-06-22T07:32:59.389681Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T07:32:59.392956Z","iopub.execute_input":"2024-06-22T07:32:59.393292Z","iopub.status.idle":"2024-06-22T07:32:59.411575Z","shell.execute_reply.started":"2024-06-22T07:32:59.393262Z","shell.execute_reply":"2024-06-22T07:32:59.410602Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"  essay_id                                          full_text  score  label\n0  a39e44d  Do you think that the face was created by alie...      2      1\n1  250a96a  In 1976, NASA's Viking 1 spacecraft snapped a ...      3      2\n2  3b20650  There are numerous mysteries of the world that...      6      5\n3  18327f9  I think that smart cars should not be on the r...      2      1\n4  a3e37e8  It's come to my attention that more and more p...      4      3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>full_text</th>\n      <th>score</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>a39e44d</td>\n      <td>Do you think that the face was created by alie...</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250a96a</td>\n      <td>In 1976, NASA's Viking 1 spacecraft snapped a ...</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3b20650</td>\n      <td>There are numerous mysteries of the world that...</td>\n      <td>6</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18327f9</td>\n      <td>I think that smart cars should not be on the r...</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a3e37e8</td>\n      <td>It's come to my attention that more and more p...</td>\n      <td>4</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-22T07:32:59.412892Z","iopub.execute_input":"2024-06-22T07:32:59.413263Z","iopub.status.idle":"2024-06-22T07:32:59.429254Z","shell.execute_reply.started":"2024-06-22T07:32:59.413226Z","shell.execute_reply":"2024-06-22T07:32:59.428087Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"label\n2    1468\n1    1089\n3     873\n0     297\n4     232\n5      41\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.model_ckpt)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n\ndef tokenize(batch):\n    tokenized_inputs = tokenizer(\n        batch['full_text'],\n        padding=False,\n        truncation=True,\n        max_length=CFG.max_input_length,\n    )\n    return tokenized_inputs\n\n\n# return the original model or model with freezed layers or peft or second and third\ndef model_init():\n    ### Load model from checkpoint\n    model = AutoModelForSequenceClassification.from_pretrained(\n        CFG.model_ckpt,\n        num_labels=CFG.n_labels,\n    ).to(CFG.device)\n    \n    # Freeze layers\n    if CFG.n_freeze:\n        # embedding layer\n        for param in model.base_model.embeddings.parameters():\n            param.requires_grad = False\n            \n        # eack encoder layer\n        for i in range(CFG.n_freeze):\n            for param in model.base_model.encoder.layer[i].parameters():\n                param.requires_grad = False\n                \n    \n    \n    # Create PEFT (LoRA) model\n    if CFG.use_peft:\n        peft_config = LoraConfig(\n            task_type=TaskType.SEQ_CLS,\n            inference_mode=False,\n            use_rslora=True,\n            r=CFG.rank,\n            lora_alpha=8,\n            lora_dropout=0,\n           # target_modules = \"classifier\"\n        )\n        model = get_peft_model(model, peft_config)\n        \n    \n    return model\n\n\ndef compute_metrics(outputs):\n    predictions, labels = outputs\n    preds = np.argmax(predictions, axis=-1)\n#     print(f\"Predictions: {preds[:10]}\")\n#     print(f\"Labels: {labels[:10]}\")\n    qwk = cohen_kappa_score(\n        y1=labels, y2=preds,\n        labels=range(CFG.n_labels),\n        weights='quadratic'\n    )\n    return {'qwk': qwk}\n\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-22T07:32:59.430716Z","iopub.execute_input":"2024-06-22T07:32:59.431117Z","iopub.status.idle":"2024-06-22T07:33:01.515282Z","shell.execute_reply.started":"2024-06-22T07:32:59.431079Z","shell.execute_reply":"2024-06-22T07:33:01.514475Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91f2796b74984ddd85d886fba5373f21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1238ee6c6ff841b698d5268c7234e6ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db7b7f23ce49473aa2b1980b236d923a"}},"metadata":{}}]},{"cell_type":"code","source":"def print_trainable_params(model):\n    trainable_params = 0\n    all_params = 0\n    for _, param in model.named_parameters():\n        all_params += param.numel()\n        if param.requires_grad == True:\n            trainable_params += param.numel()\n    \n    print(f\"trainable parameters: {trainable_params}, all parameters: {all_params}, ratio: {100 * trainable_params / all_params}%\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T07:33:01.516333Z","iopub.execute_input":"2024-06-22T07:33:01.516630Z","iopub.status.idle":"2024-06-22T07:33:01.522064Z","shell.execute_reply.started":"2024-06-22T07:33:01.516605Z","shell.execute_reply":"2024-06-22T07:33:01.521040Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = model_init()\nprint_trainable_params(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T07:33:01.525418Z","iopub.execute_input":"2024-06-22T07:33:01.525764Z","iopub.status.idle":"2024-06-22T07:33:04.578645Z","shell.execute_reply.started":"2024-06-22T07:33:01.525740Z","shell.execute_reply":"2024-06-22T07:33:04.577688Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e71ebcabb2e41c099731d5ee9783057"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable parameters: 184426758, all parameters: 184426758, ratio: 100.0%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Print all module names\nfor name, module in model.named_modules():\n    print(name)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T07:33:04.579937Z","iopub.execute_input":"2024-06-22T07:33:04.580327Z","iopub.status.idle":"2024-06-22T07:33:04.588970Z","shell.execute_reply.started":"2024-06-22T07:33:04.580293Z","shell.execute_reply":"2024-06-22T07:33:04.587945Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\ndeberta\ndeberta.embeddings\ndeberta.embeddings.word_embeddings\ndeberta.embeddings.LayerNorm\ndeberta.embeddings.dropout\ndeberta.encoder\ndeberta.encoder.layer\ndeberta.encoder.layer.0\ndeberta.encoder.layer.0.attention\ndeberta.encoder.layer.0.attention.self\ndeberta.encoder.layer.0.attention.self.query_proj\ndeberta.encoder.layer.0.attention.self.key_proj\ndeberta.encoder.layer.0.attention.self.value_proj\ndeberta.encoder.layer.0.attention.self.pos_dropout\ndeberta.encoder.layer.0.attention.self.dropout\ndeberta.encoder.layer.0.attention.output\ndeberta.encoder.layer.0.attention.output.dense\ndeberta.encoder.layer.0.attention.output.LayerNorm\ndeberta.encoder.layer.0.attention.output.dropout\ndeberta.encoder.layer.0.intermediate\ndeberta.encoder.layer.0.intermediate.dense\ndeberta.encoder.layer.0.intermediate.intermediate_act_fn\ndeberta.encoder.layer.0.output\ndeberta.encoder.layer.0.output.dense\ndeberta.encoder.layer.0.output.LayerNorm\ndeberta.encoder.layer.0.output.dropout\ndeberta.encoder.layer.1\ndeberta.encoder.layer.1.attention\ndeberta.encoder.layer.1.attention.self\ndeberta.encoder.layer.1.attention.self.query_proj\ndeberta.encoder.layer.1.attention.self.key_proj\ndeberta.encoder.layer.1.attention.self.value_proj\ndeberta.encoder.layer.1.attention.self.pos_dropout\ndeberta.encoder.layer.1.attention.self.dropout\ndeberta.encoder.layer.1.attention.output\ndeberta.encoder.layer.1.attention.output.dense\ndeberta.encoder.layer.1.attention.output.LayerNorm\ndeberta.encoder.layer.1.attention.output.dropout\ndeberta.encoder.layer.1.intermediate\ndeberta.encoder.layer.1.intermediate.dense\ndeberta.encoder.layer.1.intermediate.intermediate_act_fn\ndeberta.encoder.layer.1.output\ndeberta.encoder.layer.1.output.dense\ndeberta.encoder.layer.1.output.LayerNorm\ndeberta.encoder.layer.1.output.dropout\ndeberta.encoder.layer.2\ndeberta.encoder.layer.2.attention\ndeberta.encoder.layer.2.attention.self\ndeberta.encoder.layer.2.attention.self.query_proj\ndeberta.encoder.layer.2.attention.self.key_proj\ndeberta.encoder.layer.2.attention.self.value_proj\ndeberta.encoder.layer.2.attention.self.pos_dropout\ndeberta.encoder.layer.2.attention.self.dropout\ndeberta.encoder.layer.2.attention.output\ndeberta.encoder.layer.2.attention.output.dense\ndeberta.encoder.layer.2.attention.output.LayerNorm\ndeberta.encoder.layer.2.attention.output.dropout\ndeberta.encoder.layer.2.intermediate\ndeberta.encoder.layer.2.intermediate.dense\ndeberta.encoder.layer.2.intermediate.intermediate_act_fn\ndeberta.encoder.layer.2.output\ndeberta.encoder.layer.2.output.dense\ndeberta.encoder.layer.2.output.LayerNorm\ndeberta.encoder.layer.2.output.dropout\ndeberta.encoder.layer.3\ndeberta.encoder.layer.3.attention\ndeberta.encoder.layer.3.attention.self\ndeberta.encoder.layer.3.attention.self.query_proj\ndeberta.encoder.layer.3.attention.self.key_proj\ndeberta.encoder.layer.3.attention.self.value_proj\ndeberta.encoder.layer.3.attention.self.pos_dropout\ndeberta.encoder.layer.3.attention.self.dropout\ndeberta.encoder.layer.3.attention.output\ndeberta.encoder.layer.3.attention.output.dense\ndeberta.encoder.layer.3.attention.output.LayerNorm\ndeberta.encoder.layer.3.attention.output.dropout\ndeberta.encoder.layer.3.intermediate\ndeberta.encoder.layer.3.intermediate.dense\ndeberta.encoder.layer.3.intermediate.intermediate_act_fn\ndeberta.encoder.layer.3.output\ndeberta.encoder.layer.3.output.dense\ndeberta.encoder.layer.3.output.LayerNorm\ndeberta.encoder.layer.3.output.dropout\ndeberta.encoder.layer.4\ndeberta.encoder.layer.4.attention\ndeberta.encoder.layer.4.attention.self\ndeberta.encoder.layer.4.attention.self.query_proj\ndeberta.encoder.layer.4.attention.self.key_proj\ndeberta.encoder.layer.4.attention.self.value_proj\ndeberta.encoder.layer.4.attention.self.pos_dropout\ndeberta.encoder.layer.4.attention.self.dropout\ndeberta.encoder.layer.4.attention.output\ndeberta.encoder.layer.4.attention.output.dense\ndeberta.encoder.layer.4.attention.output.LayerNorm\ndeberta.encoder.layer.4.attention.output.dropout\ndeberta.encoder.layer.4.intermediate\ndeberta.encoder.layer.4.intermediate.dense\ndeberta.encoder.layer.4.intermediate.intermediate_act_fn\ndeberta.encoder.layer.4.output\ndeberta.encoder.layer.4.output.dense\ndeberta.encoder.layer.4.output.LayerNorm\ndeberta.encoder.layer.4.output.dropout\ndeberta.encoder.layer.5\ndeberta.encoder.layer.5.attention\ndeberta.encoder.layer.5.attention.self\ndeberta.encoder.layer.5.attention.self.query_proj\ndeberta.encoder.layer.5.attention.self.key_proj\ndeberta.encoder.layer.5.attention.self.value_proj\ndeberta.encoder.layer.5.attention.self.pos_dropout\ndeberta.encoder.layer.5.attention.self.dropout\ndeberta.encoder.layer.5.attention.output\ndeberta.encoder.layer.5.attention.output.dense\ndeberta.encoder.layer.5.attention.output.LayerNorm\ndeberta.encoder.layer.5.attention.output.dropout\ndeberta.encoder.layer.5.intermediate\ndeberta.encoder.layer.5.intermediate.dense\ndeberta.encoder.layer.5.intermediate.intermediate_act_fn\ndeberta.encoder.layer.5.output\ndeberta.encoder.layer.5.output.dense\ndeberta.encoder.layer.5.output.LayerNorm\ndeberta.encoder.layer.5.output.dropout\ndeberta.encoder.layer.6\ndeberta.encoder.layer.6.attention\ndeberta.encoder.layer.6.attention.self\ndeberta.encoder.layer.6.attention.self.query_proj\ndeberta.encoder.layer.6.attention.self.key_proj\ndeberta.encoder.layer.6.attention.self.value_proj\ndeberta.encoder.layer.6.attention.self.pos_dropout\ndeberta.encoder.layer.6.attention.self.dropout\ndeberta.encoder.layer.6.attention.output\ndeberta.encoder.layer.6.attention.output.dense\ndeberta.encoder.layer.6.attention.output.LayerNorm\ndeberta.encoder.layer.6.attention.output.dropout\ndeberta.encoder.layer.6.intermediate\ndeberta.encoder.layer.6.intermediate.dense\ndeberta.encoder.layer.6.intermediate.intermediate_act_fn\ndeberta.encoder.layer.6.output\ndeberta.encoder.layer.6.output.dense\ndeberta.encoder.layer.6.output.LayerNorm\ndeberta.encoder.layer.6.output.dropout\ndeberta.encoder.layer.7\ndeberta.encoder.layer.7.attention\ndeberta.encoder.layer.7.attention.self\ndeberta.encoder.layer.7.attention.self.query_proj\ndeberta.encoder.layer.7.attention.self.key_proj\ndeberta.encoder.layer.7.attention.self.value_proj\ndeberta.encoder.layer.7.attention.self.pos_dropout\ndeberta.encoder.layer.7.attention.self.dropout\ndeberta.encoder.layer.7.attention.output\ndeberta.encoder.layer.7.attention.output.dense\ndeberta.encoder.layer.7.attention.output.LayerNorm\ndeberta.encoder.layer.7.attention.output.dropout\ndeberta.encoder.layer.7.intermediate\ndeberta.encoder.layer.7.intermediate.dense\ndeberta.encoder.layer.7.intermediate.intermediate_act_fn\ndeberta.encoder.layer.7.output\ndeberta.encoder.layer.7.output.dense\ndeberta.encoder.layer.7.output.LayerNorm\ndeberta.encoder.layer.7.output.dropout\ndeberta.encoder.layer.8\ndeberta.encoder.layer.8.attention\ndeberta.encoder.layer.8.attention.self\ndeberta.encoder.layer.8.attention.self.query_proj\ndeberta.encoder.layer.8.attention.self.key_proj\ndeberta.encoder.layer.8.attention.self.value_proj\ndeberta.encoder.layer.8.attention.self.pos_dropout\ndeberta.encoder.layer.8.attention.self.dropout\ndeberta.encoder.layer.8.attention.output\ndeberta.encoder.layer.8.attention.output.dense\ndeberta.encoder.layer.8.attention.output.LayerNorm\ndeberta.encoder.layer.8.attention.output.dropout\ndeberta.encoder.layer.8.intermediate\ndeberta.encoder.layer.8.intermediate.dense\ndeberta.encoder.layer.8.intermediate.intermediate_act_fn\ndeberta.encoder.layer.8.output\ndeberta.encoder.layer.8.output.dense\ndeberta.encoder.layer.8.output.LayerNorm\ndeberta.encoder.layer.8.output.dropout\ndeberta.encoder.layer.9\ndeberta.encoder.layer.9.attention\ndeberta.encoder.layer.9.attention.self\ndeberta.encoder.layer.9.attention.self.query_proj\ndeberta.encoder.layer.9.attention.self.key_proj\ndeberta.encoder.layer.9.attention.self.value_proj\ndeberta.encoder.layer.9.attention.self.pos_dropout\ndeberta.encoder.layer.9.attention.self.dropout\ndeberta.encoder.layer.9.attention.output\ndeberta.encoder.layer.9.attention.output.dense\ndeberta.encoder.layer.9.attention.output.LayerNorm\ndeberta.encoder.layer.9.attention.output.dropout\ndeberta.encoder.layer.9.intermediate\ndeberta.encoder.layer.9.intermediate.dense\ndeberta.encoder.layer.9.intermediate.intermediate_act_fn\ndeberta.encoder.layer.9.output\ndeberta.encoder.layer.9.output.dense\ndeberta.encoder.layer.9.output.LayerNorm\ndeberta.encoder.layer.9.output.dropout\ndeberta.encoder.layer.10\ndeberta.encoder.layer.10.attention\ndeberta.encoder.layer.10.attention.self\ndeberta.encoder.layer.10.attention.self.query_proj\ndeberta.encoder.layer.10.attention.self.key_proj\ndeberta.encoder.layer.10.attention.self.value_proj\ndeberta.encoder.layer.10.attention.self.pos_dropout\ndeberta.encoder.layer.10.attention.self.dropout\ndeberta.encoder.layer.10.attention.output\ndeberta.encoder.layer.10.attention.output.dense\ndeberta.encoder.layer.10.attention.output.LayerNorm\ndeberta.encoder.layer.10.attention.output.dropout\ndeberta.encoder.layer.10.intermediate\ndeberta.encoder.layer.10.intermediate.dense\ndeberta.encoder.layer.10.intermediate.intermediate_act_fn\ndeberta.encoder.layer.10.output\ndeberta.encoder.layer.10.output.dense\ndeberta.encoder.layer.10.output.LayerNorm\ndeberta.encoder.layer.10.output.dropout\ndeberta.encoder.layer.11\ndeberta.encoder.layer.11.attention\ndeberta.encoder.layer.11.attention.self\ndeberta.encoder.layer.11.attention.self.query_proj\ndeberta.encoder.layer.11.attention.self.key_proj\ndeberta.encoder.layer.11.attention.self.value_proj\ndeberta.encoder.layer.11.attention.self.pos_dropout\ndeberta.encoder.layer.11.attention.self.dropout\ndeberta.encoder.layer.11.attention.output\ndeberta.encoder.layer.11.attention.output.dense\ndeberta.encoder.layer.11.attention.output.LayerNorm\ndeberta.encoder.layer.11.attention.output.dropout\ndeberta.encoder.layer.11.intermediate\ndeberta.encoder.layer.11.intermediate.dense\ndeberta.encoder.layer.11.intermediate.intermediate_act_fn\ndeberta.encoder.layer.11.output\ndeberta.encoder.layer.11.output.dense\ndeberta.encoder.layer.11.output.LayerNorm\ndeberta.encoder.layer.11.output.dropout\ndeberta.encoder.rel_embeddings\ndeberta.encoder.LayerNorm\npooler\npooler.dense\npooler.dropout\nclassifier\ndropout\n","output_type":"stream"}]},{"cell_type":"code","source":"### Set seed\nseed_everything(CFG.seed)\n\n### Cross Validation\nskf = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)\nfor fold, (tr_idx, va_idx) in enumerate(skf.split(df, df['label'])):\n    # Split train/valid\n    df_train = df.loc[tr_idx, ['full_text', 'label']].copy()\n    df_valid = df.loc[va_idx, ['full_text', 'label']].copy()\n    print('#'*25, f\"Fold {fold}\", '#'*25)\n    # Prepare PyArrow dataset\n    ds_train = datasets.Dataset.from_pandas(df_train)\n    ds_valid = datasets.Dataset.from_pandas(df_valid)\n    # Tokenize\n    tokenized_ds_train = ds_train.map(tokenize, batched=True, batch_size=None)\n    tokenized_ds_valid = ds_valid.map(tokenize, batched=True, batch_size=None)\n    # Convert dataset's format: List -> Torch\n    tokenized_ds_train.set_format('torch')\n    tokenized_ds_valid.set_format('torch')\n    # Train\n    training_args = TrainingArguments(\n        output_dir='/kaggle/temp/',\n        overwrite_output_dir=True,\n        learning_rate=CFG.learning_rate,\n        warmup_ratio=CFG.warmup_ratio,\n        num_train_epochs=CFG.n_epochs,\n        per_device_train_batch_size=CFG.train_batch_size,\n        per_device_eval_batch_size=CFG.eval_batch_size,\n        gradient_accumulation_steps=CFG.grad_accum_steps,\n        gradient_checkpointing=True,\n        fp16=CFG.fp16,\n        logging_strategy='steps',\n        logging_steps=CFG.steps,\n        evaluation_strategy='steps',\n        eval_steps=CFG.steps,\n        save_strategy='steps',\n        save_steps=CFG.steps,\n        save_total_limit=1,\n        load_best_model_at_end=True,\n        report_to='none',\n        seed=CFG.seed,\n        )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_ds_train,\n        eval_dataset=tokenized_ds_valid,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n    #model.config.use_cache = False \n    trainer.train()\n    \n    # for debuaging and testing\n    if fold > 2:\n        break","metadata":{"execution":{"iopub.status.busy":"2024-06-22T07:33:04.590291Z","iopub.execute_input":"2024-06-22T07:33:04.590556Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"######################### Fold 0 #########################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97ba37613fb42e0b73db1d127d8ffd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"182b5f191c444d3b83e0ae3ca1a88090"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='374' max='374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [374/374 18:09, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Qwk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.201600</td>\n      <td>0.975503</td>\n      <td>0.722482</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"######################### Fold 1 #########################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42d1d558cab3464b80fc07539f66a4f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f65f45965be44de6ad5147ad6425081a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='374' max='374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [374/374 18:20, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Qwk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.969500</td>\n      <td>0.882938</td>\n      <td>0.784317</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"######################### Fold 2 #########################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f2764e127104cefab87c05fe2dd2f85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec6a5ab6038f42b596de90106b58f03d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='374' max='374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [374/374 18:13, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Qwk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.843600</td>\n      <td>0.817994</td>\n      <td>0.814450</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"######################### Fold 3 #########################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"721d34ea956a41d0b0bc9a8f1c218927"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2b65e5691774d359c87d0d614496af5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='374' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  6/374 00:10 < 16:03, 0.38 it/s, Epoch 0.03/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"#trainer.save_model(\"peft_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model(\"full_finetuned_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## which layers are used much in full fine-tune ?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}