{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8029842,"sourceType":"datasetVersion","datasetId":4732809},{"sourceId":170434135,"sourceType":"kernelVersion"},{"sourceId":170531930,"sourceType":"kernelVersion"},{"sourceId":11357,"sourceType":"modelInstanceVersion","modelInstanceId":5305}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Now i test some works and test some approaches if it work well the seconde step will be focused in the code to be in a good way.","metadata":{"execution":{"iopub.status.busy":"2024-04-22T15:14:05.441721Z","iopub.execute_input":"2024-04-22T15:14:05.44298Z","iopub.status.idle":"2024-04-22T15:14:15.260559Z","shell.execute_reply.started":"2024-04-22T15:14:05.442931Z","shell.execute_reply":"2024-04-22T15:14:15.259415Z"}}},{"cell_type":"code","source":"# Importing necessary libraries\nimport gc\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport nltk\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport random\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import cohen_kappa_score\n#from autogluon.tabular.models import NNFastAiTabularModel\n#from autogluon.tabular import TabularDataset, TabularPredictor\nfrom lightgbm import log_evaluation, early_stopping\nfrom sklearn.linear_model import SGDClassifier\nimport polars as pl\nimport torch\nfrom IPython.display import display\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T13:09:07.107036Z","iopub.execute_input":"2024-05-01T13:09:07.107625Z","iopub.status.idle":"2024-05-01T13:09:37.768338Z","shell.execute_reply.started":"2024-05-01T13:09:07.107588Z","shell.execute_reply":"2024-05-01T13:09:37.766704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [  \n    (\n        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n    ),\n]\nPATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n# 载入训练集和测试集，同时对full_text数据使用\\n\\n字符分割为列表，重命名为paragraph\n# Load training and testing sets, while using \\ n \\ n character segmentation to list and renaming to paragraph for full_text data\ntrain = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\n# for test only\n#train = train.sample(500)\ntest = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n# 显示训练集中的第一个样本数据\n# Display the first sample data in the training set\ntrain.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T13:09:37.770967Z","iopub.execute_input":"2024-05-01T13:09:37.772326Z","iopub.status.idle":"2024-05-01T13:09:38.705422Z","shell.execute_reply.started":"2024-05-01T13:09:37.772252Z","shell.execute_reply":"2024-05-01T13:09:38.704363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-01T13:09:38.707563Z","iopub.execute_input":"2024-05-01T13:09:38.708134Z","iopub.status.idle":"2024-05-01T13:09:38.717344Z","shell.execute_reply.started":"2024-05-01T13:09:38.708083Z","shell.execute_reply":"2024-05-01T13:09:38.715742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocessing**","metadata":{}},{"cell_type":"code","source":"def removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\ndef dataPreprocessing(x):\n    # 将单词转化为小写\n    # Convert words to lowercase\n    x = x.lower()\n    # Remove HTML\n    # 移除html\n    x = removeHTML(x)\n    # 删除以@作为首字母的字符串\n    # Delete strings starting with @\n    x = re.sub(\"@\\w+\", '',x)\n    # 删除数字\n    # Delete Numbers\n    x = re.sub(\"'\\d+\", '',x) # can delete it\n    x = re.sub(\"\\d+\", '',x)\n    # 删除网址\n    # Delete URL\n    x = re.sub(\"http\\w+\", '',x)\n    # 将连续空白符替换为一个空格字符\n    # Replace consecutive empty spaces with a single space character\n    x = re.sub(r\"\\s+\", \" \", x)\n    # 替换连续的句号和逗号为一个\n    # Replace consecutive commas and periods with one comma and period character\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    # 去除开头结尾的空白符\n    # Remove empty characters at the beginning and end\n    x = x.strip()\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-05-01T13:09:38.721689Z","iopub.execute_input":"2024-05-01T13:09:38.722312Z","iopub.status.idle":"2024-05-01T13:09:38.738200Z","shell.execute_reply.started":"2024-05-01T13:09:38.722239Z","shell.execute_reply":"2024-05-01T13:09:38.736553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.Feature engineering\n   - paragraph\n   - senetence\n   - word","metadata":{}},{"cell_type":"markdown","source":"### Features engineering - https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments","metadata":{}},{"cell_type":"code","source":"# 段落特征\n# paragraph features\ndef Paragraph_Preprocess(tmp):\n    # 将段落列表扩展为一行行的数据\n    # Expand the paragraph list into several lines of data\n    tmp = tmp.explode('paragraph')\n    # 段落预处理\n    # Paragraph preprocessing\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing,return_dtype=str))\n    # 计算每一个段落的长度\n    # Calculate the length of each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x),return_dtype=int).alias(\"paragraph_len\"))\n    # 计算每一个段落中句子的数量和单词的数量\n    # Calculate the number of sentences and words in each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.')),return_dtype=int).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' ')),return_dtype=int).alias(\"paragraph_word_cnt\"),)\n    return tmp\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\ndef Paragraph_Eng(train_tmp):\n    aggs = [\n        # 统计段落长度大于和小于 i 值的个数\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n        # 其他\n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\n\n# sentence feature\ndef Sentence_Preprocess(tmp):\n    # 对full_text预处理，并且使用句号分割出文本的句子\n    # Preprocess full_text and use periods to segment sentences in the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing,return_dtype=str).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    # 计算句子的长度\n    # Calculate the length of a sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x),return_dtype=int).alias(\"sentence_len\"))\n    # 筛选出句子长度大于15的那一部分数据\n    # Filter out the portion of data with a sentence length greater than 15\n    tmp = tmp.filter(pl.col('sentence_len')>=15)\n    # 统计每一句中单词的数量\n    # Count the number of words in each sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' ')),return_dtype=int).alias(\"sentence_word_cnt\"))\n    \n    return tmp\n# feature_eng\ndef Sentence_Eng(train_tmp):\n    sentence_fea = ['sentence_len','sentence_word_cnt']\n    aggs = [\n        # 统计句子长度大于 i 的句子个数\n        # Count the number of sentences with a length greater than i\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n        # 其他\n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\n\n# word feature\ndef Word_Preprocess(tmp):\n    # 对full_text预处理，并且使用空格符分割出文本的单词\n    # Preprocess full_text and use spaces to separate words from the text\n    #  train.with_columns(pl.col('full_text').map_elements(dataPreprocessing,return_dtype=str))\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing,return_dtype=str).str.split(by=\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    # 计算每一个的单词长度\n    # Calculate the length of each word\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x),return_dtype=int).alias(\"word_len\"))\n    # 删除单词长度为0的数据\n    # Delete data with a word length of 0\n    tmp = tmp.filter(pl.col('word_len')!=0)\n    \n    return tmp\n# feature_eng\ndef Word_Eng(train_tmp):\n    aggs = [\n        # 统计单词长度大于 i+1 的单词个数\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n        # 其他\n        # other\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T13:09:38.741138Z","iopub.execute_input":"2024-05-01T13:09:38.741783Z","iopub.status.idle":"2024-05-01T13:09:38.782845Z","shell.execute_reply.started":"2024-05-01T13:09:38.741731Z","shell.execute_reply":"2024-05-01T13:09:38.781803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# implement function apply_preprocessing to apply all preprocessing\n# implement function apply_feature_eng to apply all feature engineering \n\ndef apply_preprocessing_featureEng(df):\n    \n    # Paragraph\n    tmp = Paragraph_Preprocess(df)\n    train_feats = Paragraph_Eng(tmp)\n    train_feats['score'] = df['score']\n    \n    # sentences\n    tmp = Sentence_Preprocess(df)\n    # Merge the newly generated feature data with the previously generated feature data\n    train_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n    \n    # word\n    tmp = Word_Preprocess(df)\n    # Merge the newly generated feature data with the previously generated feature data\n    train_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n    \n    \n  #  feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n  #  print('Features Number: ',len(feature_names))\n  #  display(train_feats.head(3))\n    \n    return train_feats\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-01T13:09:38.784367Z","iopub.execute_input":"2024-05-01T13:09:38.785428Z","iopub.status.idle":"2024-05-01T13:09:38.794175Z","shell.execute_reply.started":"2024-05-01T13:09:38.785386Z","shell.execute_reply":"2024-05-01T13:09:38.792858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare the data\n# we may have some data leakage in the feature eng steps but it is acceptable\ntrain_feats = apply_preprocessing_featureEng(train)\ntrain_feats = train_feats.merge(train[['essay_id','full_text']].to_pandas(), on='essay_id', how='left') # full text to use in the pipeline\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T13:09:38.796118Z","iopub.execute_input":"2024-05-01T13:09:38.796537Z","iopub.status.idle":"2024-05-01T13:10:10.934177Z","shell.execute_reply.started":"2024-05-01T13:09:38.796492Z","shell.execute_reply":"2024-05-01T13:10:10.932913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TfidfVectorizer parameter\nvectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(1,4),\n            min_df=0.05,\n            max_df=0.95,\n            sublinear_tf=True,\n)\n\n# column transformer\nvectorizar_trans = ColumnTransformer(\n    [(\"vectorizer\",vectorizer,'full_text')],\n    remainder = 'passthrough')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T13:10:10.935966Z","iopub.execute_input":"2024-05-01T13:10:10.936557Z","iopub.status.idle":"2024-05-01T13:10:10.944769Z","shell.execute_reply.started":"2024-05-01T13:10:10.936504Z","shell.execute_reply":"2024-05-01T13:10:10.943123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = vectorizar_trans.fit_transform(train_feats.drop(['essay_id','score'], axis = 1))\nfeatures","metadata":{"execution":{"iopub.status.busy":"2024-05-01T13:10:10.946812Z","iopub.execute_input":"2024-05-01T13:10:10.947641Z","iopub.status.idle":"2024-05-01T13:12:54.563654Z","shell.execute_reply.started":"2024-05-01T13:10:10.947573Z","shell.execute_reply":"2024-05-01T13:12:54.562617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install dill\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T13:27:31.649689Z","iopub.execute_input":"2024-05-01T13:27:31.650237Z","iopub.status.idle":"2024-05-01T13:28:07.361903Z","shell.execute_reply.started":"2024-05-01T13:27:31.650198Z","shell.execute_reply":"2024-05-01T13:28:07.359894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import dill\n\n# Save TF-IDF Vectorizer\nwith open('tfidf_vectorizer_fitted.pkl', 'wb') as f:\n    dill.dump(vectorizar_trans, f)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T13:29:31.504463Z","iopub.execute_input":"2024-05-01T13:29:31.504887Z","iopub.status.idle":"2024-05-01T13:29:33.050747Z","shell.execute_reply.started":"2024-05-01T13:29:31.504858Z","shell.execute_reply":"2024-05-01T13:29:33.049882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df = pd.DataFrame.sparse.from_spmatrix(features)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T16:08:30.702308Z","iopub.execute_input":"2024-04-29T16:08:30.702929Z","iopub.status.idle":"2024-04-29T16:08:31.248449Z","shell.execute_reply.started":"2024-04-29T16:08:30.702898Z","shell.execute_reply":"2024-04-29T16:08:31.247410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df['essay_id'] = train_feats['essay_id']","metadata":{"execution":{"iopub.status.busy":"2024-04-29T16:08:33.094073Z","iopub.execute_input":"2024-04-29T16:08:33.094811Z","iopub.status.idle":"2024-04-29T16:08:33.112295Z","shell.execute_reply.started":"2024-04-29T16:08:33.094778Z","shell.execute_reply":"2024-04-29T16:08:33.111433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T16:08:40.571868Z","iopub.execute_input":"2024-04-29T16:08:40.572243Z","iopub.status.idle":"2024-04-29T16:08:40.922626Z","shell.execute_reply.started":"2024-04-29T16:08:40.572215Z","shell.execute_reply":"2024-04-29T16:08:40.921755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_df.to_csv(\"features_eng_tfidf.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T16:08:45.700260Z","iopub.execute_input":"2024-04-29T16:08:45.701003Z","iopub.status.idle":"2024-04-29T16:10:15.768972Z","shell.execute_reply.started":"2024-04-29T16:08:45.700960Z","shell.execute_reply":"2024-04-29T16:10:15.768205Z"},"trusted":true},"execution_count":null,"outputs":[]}]}