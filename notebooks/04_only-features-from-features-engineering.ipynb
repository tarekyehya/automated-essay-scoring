{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7f9d7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-22T15:14:05.44298Z",
     "iopub.status.busy": "2024-04-22T15:14:05.441721Z",
     "iopub.status.idle": "2024-04-22T15:14:15.260559Z",
     "shell.execute_reply": "2024-04-22T15:14:15.259415Z",
     "shell.execute_reply.started": "2024-04-22T15:14:05.442931Z"
    },
    "papermill": {
     "duration": 0.005707,
     "end_time": "2024-05-02T20:27:06.584366",
     "exception": false,
     "start_time": "2024-05-02T20:27:06.578659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Now i test some works and test some approaches if it work well the seconde step will be focused in the code to be in a good way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "364ddbd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T20:27:06.597026Z",
     "iopub.status.busy": "2024-05-02T20:27:06.596211Z",
     "iopub.status.idle": "2024-05-02T20:27:35.386706Z",
     "shell.execute_reply": "2024-05-02T20:27:35.385261Z"
    },
    "papermill": {
     "duration": 28.800183,
     "end_time": "2024-05-02T20:27:35.389474",
     "exception": false,
     "start_time": "2024-05-02T20:27:06.589291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import nltk\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "#from autogluon.tabular.models import NNFastAiTabularModel\n",
    "#from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from lightgbm import log_evaluation, early_stopping\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import polars as pl\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import dill\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8077eb42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T20:27:35.402658Z",
     "iopub.status.busy": "2024-05-02T20:27:35.401515Z",
     "iopub.status.idle": "2024-05-02T20:27:36.190209Z",
     "shell.execute_reply": "2024-05-02T20:27:36.189002Z"
    },
    "papermill": {
     "duration": 0.79772,
     "end_time": "2024-05-02T20:27:36.192686",
     "exception": false,
     "start_time": "2024-05-02T20:27:35.394966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>essay_id</th><th>full_text</th><th>score</th><th>paragraph</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>list[str]</td></tr></thead><tbody><tr><td>&quot;000d118&quot;</td><td>&quot;Many people ha…</td><td>3</td><td>[&quot;Many people have car where they live. The thing they don&#x27;t know is that when you use a car alot of thing can happen like you can get in accidet or the smoke that the car has is bad to breath on if someone is walk but in VAUBAN,Germany they dont have that proble because 70 percent of vauban&#x27;s families do not own cars,and 57 percent sold a car to move there. Street parkig ,driveways and home garages are forbidden on the outskirts of freiburd that near the French and Swiss borders. You probaly won&#x27;t see a car in Vauban&#x27;s streets because they are completely &quot;car free&quot; but If some that lives in VAUBAN that owns a car ownership is allowed,but there are only two places that you can park a large garages at the edge of the development,where a car owner buys a space but it not cheap to buy one they sell the space for you car for $40,000 along with a home. The vauban people completed this in 2006 ,they said that this an example of a growing trend in Europe,The untile states and some where else are suburban life from auto use this is called &quot;smart planning&quot;. The current efforts to drastically reduce greenhouse gas emissions from tailes the passengee cars are responsible for 12 percent of greenhouse gas emissions in Europe and up to 50 percent in some car intensive in the United States. I honeslty think that good idea that they did that is Vaudan because that makes cities denser and better for walking and in VAUBAN there are 5,500 residents within a rectangular square mile. In the artical David Gold berg said that &quot;All of our development since World war 2 has been centered on the cars,and that will have to change&quot; and i think that was very true what David Gold said because alot thing we need cars to do we can go anyway were with out cars beacuse some people are a very lazy to walk to place thats why they alot of people use car and i think that it was a good idea that that they did that in VAUBAN so people can see how we really don&#x27;t need car to go to place from place because we can walk from were we need to go or we can ride bycles with out the use of a car. It good that they are doing that if you thik about your help the earth in way and thats a very good thing to. In the United states ,the Environmental protection Agency is promoting what is called &quot;car reduced&quot;communtunties,and the legislators are starting to act,if cautiously. Maany experts expect pubic transport serving suburbs to play a much larger role in a new six years federal transportation bill to approved this year. In previous bill,80 percent of appropriations have by law gone to highways and only 20 percent to other transports. There many good reason why they should do this.    &quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 4)\n",
       "┌──────────┬───────────────────────────────────┬───────┬───────────────────────────────────┐\n",
       "│ essay_id ┆ full_text                         ┆ score ┆ paragraph                         │\n",
       "│ ---      ┆ ---                               ┆ ---   ┆ ---                               │\n",
       "│ str      ┆ str                               ┆ i64   ┆ list[str]                         │\n",
       "╞══════════╪═══════════════════════════════════╪═══════╪═══════════════════════════════════╡\n",
       "│ 000d118  ┆ Many people have car where they … ┆ 3     ┆ [\"Many people have car where the… │\n",
       "└──────────┴───────────────────────────────────┴───────┴───────────────────────────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [  \n",
    "    (\n",
    "        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n",
    "    ),\n",
    "]\n",
    "PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n",
    "# 载入训练集和测试集，同时对full_text数据使用\\n\\n字符分割为列表，重命名为paragraph\n",
    "# Load training and testing sets, while using \\ n \\ n character segmentation to list and renaming to paragraph for full_text data\n",
    "train = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\n",
    "# for test only\n",
    "#train = train.sample(500)\n",
    "test = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n",
    "# 显示训练集中的第一个样本数据\n",
    "# Display the first sample data in the training set\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82455a26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T20:27:36.204639Z",
     "iopub.status.busy": "2024-05-02T20:27:36.204268Z",
     "iopub.status.idle": "2024-05-02T20:27:36.210643Z",
     "shell.execute_reply": "2024-05-02T20:27:36.209326Z"
    },
    "papermill": {
     "duration": 0.015609,
     "end_time": "2024-05-02T20:27:36.213387",
     "exception": false,
     "start_time": "2024-05-02T20:27:36.197778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17307, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1006a1a0",
   "metadata": {
    "papermill": {
     "duration": 0.005382,
     "end_time": "2024-05-02T20:27:36.224681",
     "exception": false,
     "start_time": "2024-05-02T20:27:36.219299",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94e2d576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T20:27:36.237235Z",
     "iopub.status.busy": "2024-05-02T20:27:36.236594Z",
     "iopub.status.idle": "2024-05-02T20:27:36.244114Z",
     "shell.execute_reply": "2024-05-02T20:27:36.243188Z"
    },
    "papermill": {
     "duration": 0.016367,
     "end_time": "2024-05-02T20:27:36.246271",
     "exception": false,
     "start_time": "2024-05-02T20:27:36.229904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def removeHTML(x):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',x)\n",
    "def dataPreprocessing(x):\n",
    "    # 将单词转化为小写\n",
    "    # Convert words to lowercase\n",
    "    x = x.lower()\n",
    "    # Remove HTML\n",
    "    # 移除html\n",
    "    x = removeHTML(x)\n",
    "    # 删除以@作为首字母的字符串\n",
    "    # Delete strings starting with @\n",
    "    x = re.sub(\"@\\w+\", '',x)\n",
    "    # 删除数字\n",
    "    # Delete Numbers\n",
    "    x = re.sub(\"'\\d+\", '',x) # can delete it\n",
    "    x = re.sub(\"\\d+\", '',x)\n",
    "    # 删除网址\n",
    "    # Delete URL\n",
    "    x = re.sub(\"http\\w+\", '',x)\n",
    "    # 将连续空白符替换为一个空格字符\n",
    "    # Replace consecutive empty spaces with a single space character\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    # 替换连续的句号和逗号为一个\n",
    "    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\.+\", \".\", x)\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    # 去除开头结尾的空白符\n",
    "    # Remove empty characters at the beginning and end\n",
    "    x = x.strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa02c87",
   "metadata": {
    "papermill": {
     "duration": 0.004972,
     "end_time": "2024-05-02T20:27:36.256436",
     "exception": false,
     "start_time": "2024-05-02T20:27:36.251464",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.Feature engineering\n",
    "   - paragraph\n",
    "   - senetence\n",
    "   - word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd39338",
   "metadata": {
    "papermill": {
     "duration": 0.004873,
     "end_time": "2024-05-02T20:27:36.266444",
     "exception": false,
     "start_time": "2024-05-02T20:27:36.261571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Features engineering - https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7764530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T20:27:36.279006Z",
     "iopub.status.busy": "2024-05-02T20:27:36.278477Z",
     "iopub.status.idle": "2024-05-02T20:27:36.303434Z",
     "shell.execute_reply": "2024-05-02T20:27:36.302221Z"
    },
    "papermill": {
     "duration": 0.034282,
     "end_time": "2024-05-02T20:27:36.306110",
     "exception": false,
     "start_time": "2024-05-02T20:27:36.271828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 段落特征\n",
    "# paragraph features\n",
    "def Paragraph_Preprocess(tmp):\n",
    "    # 将段落列表扩展为一行行的数据\n",
    "    # Expand the paragraph list into several lines of data\n",
    "    tmp = tmp.explode('paragraph')\n",
    "    # 段落预处理\n",
    "    # Paragraph preprocessing\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing,return_dtype=str))\n",
    "    # 计算每一个段落的长度\n",
    "    # Calculate the length of each paragraph\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x),return_dtype=int).alias(\"paragraph_len\"))\n",
    "    # 计算每一个段落中句子的数量和单词的数量\n",
    "    # Calculate the number of sentences and words in each paragraph\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.')),return_dtype=int).alias(\"paragraph_sentence_cnt\"),\n",
    "                    pl.col('paragraph').map_elements(lambda x: len(x.split(' ')),return_dtype=int).alias(\"paragraph_word_cnt\"),)\n",
    "    return tmp\n",
    "# feature_eng\n",
    "paragraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n",
    "def Paragraph_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        # 统计段落长度大于和小于 i 值的个数\n",
    "        # Count the number of paragraph lengths greater than and less than the i-value\n",
    "        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n",
    "        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n",
    "        # 其他\n",
    "        # other\n",
    "        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n",
    "        ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "\n",
    "# sentence feature\n",
    "def Sentence_Preprocess(tmp):\n",
    "    # 对full_text预处理，并且使用句号分割出文本的句子\n",
    "    # Preprocess full_text and use periods to segment sentences in the text\n",
    "    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing,return_dtype=str).str.split(by=\".\").alias(\"sentence\"))\n",
    "    tmp = tmp.explode('sentence')\n",
    "    # 计算句子的长度\n",
    "    # Calculate the length of a sentence\n",
    "    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x),return_dtype=int).alias(\"sentence_len\"))\n",
    "    # 筛选出句子长度大于15的那一部分数据\n",
    "    # Filter out the portion of data with a sentence length greater than 15\n",
    "    tmp = tmp.filter(pl.col('sentence_len')>=15)\n",
    "    # 统计每一句中单词的数量\n",
    "    # Count the number of words in each sentence\n",
    "    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' ')),return_dtype=int).alias(\"sentence_word_cnt\"))\n",
    "    \n",
    "    return tmp\n",
    "# feature_eng\n",
    "def Sentence_Eng(train_tmp):\n",
    "    sentence_fea = ['sentence_len','sentence_word_cnt']\n",
    "    aggs = [\n",
    "        # 统计句子长度大于 i 的句子个数\n",
    "        # Count the number of sentences with a length greater than i\n",
    "        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n",
    "        # 其他\n",
    "        # other\n",
    "        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n",
    "        ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "\n",
    "# word feature\n",
    "def Word_Preprocess(tmp):\n",
    "    # 对full_text预处理，并且使用空格符分割出文本的单词\n",
    "    # Preprocess full_text and use spaces to separate words from the text\n",
    "    #  train.with_columns(pl.col('full_text').map_elements(dataPreprocessing,return_dtype=str))\n",
    "    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing,return_dtype=str).str.split(by=\" \").alias(\"word\"))\n",
    "    tmp = tmp.explode('word')\n",
    "    # 计算每一个的单词长度\n",
    "    # Calculate the length of each word\n",
    "    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x),return_dtype=int).alias(\"word_len\"))\n",
    "    # 删除单词长度为0的数据\n",
    "    # Delete data with a word length of 0\n",
    "    tmp = tmp.filter(pl.col('word_len')!=0)\n",
    "    \n",
    "    return tmp\n",
    "# feature_eng\n",
    "def Word_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        # 统计单词长度大于 i+1 的单词个数\n",
    "        # Count the number of words with a length greater than i+1\n",
    "        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n",
    "        # 其他\n",
    "        # other\n",
    "        pl.col('word_len').max().alias(f\"word_len_max\"),\n",
    "        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n",
    "        pl.col('word_len').std().alias(f\"word_len_std\"),\n",
    "        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n",
    "        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n",
    "        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n",
    "        ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cac3a43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T20:27:36.318181Z",
     "iopub.status.busy": "2024-05-02T20:27:36.317801Z",
     "iopub.status.idle": "2024-05-02T20:27:36.324956Z",
     "shell.execute_reply": "2024-05-02T20:27:36.323603Z"
    },
    "papermill": {
     "duration": 0.016101,
     "end_time": "2024-05-02T20:27:36.327431",
     "exception": false,
     "start_time": "2024-05-02T20:27:36.311330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# implement function apply_preprocessing to apply all preprocessing\n",
    "# implement function apply_feature_eng to apply all feature engineering \n",
    "\n",
    "def apply_preprocessing_featureEng(df):\n",
    "    \n",
    "    # Paragraph\n",
    "    tmp = Paragraph_Preprocess(df)\n",
    "    train_feats = Paragraph_Eng(tmp)\n",
    "    \n",
    "    # sentences\n",
    "    tmp = Sentence_Preprocess(df)\n",
    "    # Merge the newly generated feature data with the previously generated feature data\n",
    "    train_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n",
    "    \n",
    "    # word\n",
    "    tmp = Word_Preprocess(df)\n",
    "    # Merge the newly generated feature data with the previously generated feature data\n",
    "    train_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n",
    "    \n",
    "    \n",
    "  #  feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "  #  print('Features Number: ',len(feature_names))\n",
    "  #  display(train_feats.head(3))\n",
    "    \n",
    "    return train_feats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55f1ab07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T20:27:36.339600Z",
     "iopub.status.busy": "2024-05-02T20:27:36.339224Z",
     "iopub.status.idle": "2024-05-02T20:28:00.542331Z",
     "shell.execute_reply": "2024-05-02T20:28:00.541026Z"
    },
    "papermill": {
     "duration": 24.212118,
     "end_time": "2024-05-02T20:28:00.544935",
     "exception": false,
     "start_time": "2024-05-02T20:27:36.332817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>word_12_cnt</th>\n",
       "      <th>word_13_cnt</th>\n",
       "      <th>word_14_cnt</th>\n",
       "      <th>word_15_cnt</th>\n",
       "      <th>word_len_max</th>\n",
       "      <th>word_len_mean</th>\n",
       "      <th>word_len_std</th>\n",
       "      <th>word_len_q1</th>\n",
       "      <th>word_len_q2</th>\n",
       "      <th>word_len_q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>4.378819</td>\n",
       "      <td>2.538495</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4.012048</td>\n",
       "      <td>2.060968</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>4.574545</td>\n",
       "      <td>2.604621</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  word_12_cnt  word_13_cnt  \\\n",
       "0                  1                  1  ...            6            6   \n",
       "1                  3                  3  ...            0            0   \n",
       "2                  4                  4  ...           14           10   \n",
       "\n",
       "   word_14_cnt  word_15_cnt  word_len_max  word_len_mean  word_len_std  \\\n",
       "0            5            2            25       4.378819      2.538495   \n",
       "1            0            0            11       4.012048      2.060968   \n",
       "2            5            2            15       4.574545      2.604621   \n",
       "\n",
       "   word_len_q1  word_len_q2  word_len_q3  \n",
       "0          3.0          4.0          5.0  \n",
       "1          2.0          4.0          5.0  \n",
       "2          3.0          4.0          5.0  \n",
       "\n",
       "[3 rows x 70 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the data\n",
    "train_feats = apply_preprocessing_featureEng(train)\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f77edc5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T20:28:00.559412Z",
     "iopub.status.busy": "2024-05-02T20:28:00.559005Z",
     "iopub.status.idle": "2024-05-02T20:28:00.565776Z",
     "shell.execute_reply": "2024-05-02T20:28:00.564573Z"
    },
    "papermill": {
     "duration": 0.017907,
     "end_time": "2024-05-02T20:28:00.568282",
     "exception": false,
     "start_time": "2024-05-02T20:28:00.550375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TfidfVectorizer parameter\n",
    "vectorizer = TfidfVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(1,4),\n",
    "            min_df=0.05,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True,\n",
    ")\n",
    "\n",
    "# column transformer\n",
    "vectorizar_trans = ColumnTransformer(\n",
    "    [(\"vectorizer\",vectorizer,'full_text')],\n",
    "    remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abcca36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T20:28:00.581506Z",
     "iopub.status.busy": "2024-05-02T20:28:00.580627Z",
     "iopub.status.idle": "2024-05-02T20:29:50.215365Z",
     "shell.execute_reply": "2024-05-02T20:29:50.213958Z"
    },
    "papermill": {
     "duration": 109.648802,
     "end_time": "2024-05-02T20:29:50.222682",
     "exception": false,
     "start_time": "2024-05-02T20:28:00.573880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>8663</th>\n",
       "      <th>8664</th>\n",
       "      <th>8665</th>\n",
       "      <th>8666</th>\n",
       "      <th>8667</th>\n",
       "      <th>8668</th>\n",
       "      <th>8669</th>\n",
       "      <th>8670</th>\n",
       "      <th>8671</th>\n",
       "      <th>essay_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023801</td>\n",
       "      <td>0.048692</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>000d118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025332</td>\n",
       "      <td>0.01963</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>000fe60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017083</td>\n",
       "      <td>0.012842</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>001ab80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018466</td>\n",
       "      <td>0.01431</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>001bdc0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022545</td>\n",
       "      <td>0.017813</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014897</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>002ba53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1  2         3  4         5  6  7  8  9  ...  8663  8664  \\\n",
       "0         0         0  0         0  0         0  0  0  0  0  ...     0     0   \n",
       "1  0.025332   0.01963  0         0  0         0  0  0  0  0  ...     0     0   \n",
       "2  0.017083  0.012842  0         0  0         0  0  0  0  0  ...     0     0   \n",
       "3  0.018466   0.01431  0         0  0         0  0  0  0  0  ...     0     0   \n",
       "4  0.022545  0.017813  0  0.014897  0  0.022249  0  0  0  0  ...     0     0   \n",
       "\n",
       "   8665  8666      8667      8668  8669  8670  8671  essay_id  \n",
       "0     0     0  0.023801  0.048692     0     0     0   000d118  \n",
       "1     0     0         0         0     0     0     0   000fe60  \n",
       "2     0     0         0         0     0     0     0   001ab80  \n",
       "3     0     0         0         0     0     0     0   001bdc0  \n",
       "4     0     0         0         0     0     0     0   002ba53  \n",
       "\n",
       "[5 rows x 8673 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = vectorizar_trans.fit_transform(train[['full_text']].to_pandas())\n",
    "features_df = pd.DataFrame.sparse.from_spmatrix(features)\n",
    "features_df['essay_id'] = train['essay_id']\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c019d90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T20:29:50.236399Z",
     "iopub.status.busy": "2024-05-02T20:29:50.235995Z",
     "iopub.status.idle": "2024-05-02T20:29:50.765712Z",
     "shell.execute_reply": "2024-05-02T20:29:50.764537Z"
    },
    "papermill": {
     "duration": 0.540123,
     "end_time": "2024-05-02T20:29:50.768620",
     "exception": false,
     "start_time": "2024-05-02T20:29:50.228497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>8663</th>\n",
       "      <th>8664</th>\n",
       "      <th>8665</th>\n",
       "      <th>8666</th>\n",
       "      <th>8667</th>\n",
       "      <th>8668</th>\n",
       "      <th>8669</th>\n",
       "      <th>8670</th>\n",
       "      <th>8671</th>\n",
       "      <th>essay_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.023801</td>\n",
       "      <td>0.048692</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>000d118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025332</td>\n",
       "      <td>0.01963</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>000fe60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.017083</td>\n",
       "      <td>0.012842</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>001ab80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 8673 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1  2  3  4  5  6  7  8  9  ...  8663  8664  8665  8666  \\\n",
       "0         0         0  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
       "1  0.025332   0.01963  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
       "2  0.017083  0.012842  0  0  0  0  0  0  0  0  ...     0     0     0     0   \n",
       "\n",
       "       8667      8668  8669  8670  8671  essay_id  \n",
       "0  0.023801  0.048692     0     0     0   000d118  \n",
       "1         0         0     0     0     0   000fe60  \n",
       "2         0         0     0     0     0   001ab80  \n",
       "\n",
       "[3 rows x 8673 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d18b0058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T20:29:50.782648Z",
     "iopub.status.busy": "2024-05-02T20:29:50.782235Z",
     "iopub.status.idle": "2024-05-02T21:21:30.218004Z",
     "shell.execute_reply": "2024-05-02T21:21:30.216850Z"
    },
    "papermill": {
     "duration": 3099.446635,
     "end_time": "2024-05-02T21:21:30.221407",
     "exception": false,
     "start_time": "2024-05-02T20:29:50.774772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_df.to_csv(\"features_tfidf.csv\",index=False)\n",
    "train_feats.to_csv(\"features_eng.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76caf841",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T21:21:30.240257Z",
     "iopub.status.busy": "2024-05-02T21:21:30.239026Z",
     "iopub.status.idle": "2024-05-02T21:21:31.164420Z",
     "shell.execute_reply": "2024-05-02T21:21:31.163392Z"
    },
    "papermill": {
     "duration": 0.935609,
     "end_time": "2024-05-02T21:21:31.166875",
     "exception": false,
     "start_time": "2024-05-02T21:21:30.231266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save TF-IDF Vectorizer\n",
    "with open('tfidf_vectorizer_fitted.pkl', 'wb') as f:\n",
    "    dill.dump(vectorizar_trans, f)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "datasetId": 4732809,
     "sourceId": 8029842,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 170434135,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 170531930,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 5305,
     "sourceId": 11357,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3269.889323,
   "end_time": "2024-05-02T21:21:33.622069",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-02T20:27:03.732746",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
